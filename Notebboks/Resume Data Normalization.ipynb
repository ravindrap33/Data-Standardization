{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "579bf79e-e423-4836-be7e-f4a1b85835f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### describe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5f90ff1-58a7-43b0-92b3-bc6658d67d39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"hello\")\n",
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e90f17d-7788-474f-8758-20e68be9cb0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "describe raw_resume_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddec4b4d-738b-49ed-863e-9a6536edd76d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Top 10 Raw Resume Data Subset"
    }
   },
   "outputs": [],
   "source": [
    "select * from raw_resume_data limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd11e02-4026-4a4b-9d12-f4e7103b1811",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Skill Exploding"
    }
   },
   "outputs": [],
   "source": [
    "with exp_skill as (\n",
    "  select resume_id, name, job_title, skills, location, date, experience_years, skill\n",
    "  from raw_resume_data \n",
    "  lateral view explode(split(skills, ' ')) as skill\n",
    ")\n",
    "select resume_id,name,skill from exp_skill limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7250c88c-c1f4-4e17-a88b-772ceac3fd09",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Top 10 Job Titles and Skills from Resume Data"
    }
   },
   "outputs": [],
   "source": [
    "with exp_skill as (\n",
    "  select resume_id, name, job_title, skills, location, date, experience_years, skill\n",
    "  from raw_resume_data \n",
    "  lateral view explode(split(skills, ' ')) as skill\n",
    ")\n",
    "select job_title,skill,count(skill) as total_skills from exp_skill group by job_title,skill order by total_skills desc limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4ba1158-9128-448d-a73d-972484ccb4cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df = spark.table('raw_resume_data')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d71cd14f-37ca-4cac-982f-9dc6706a17ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, when\n",
    "df1 = df.withColumn(\"job_title\",\n",
    "    when(col(\"job_title\") == \"Data Engr.\", \"Data Engineer\")\n",
    "    .when(col(\"job_title\") == \"Software Dev\", \"Software Developer\")\n",
    "    .when(col(\"job_title\") == \"Proj. Manager\", \"Project Manager\")\n",
    "    .when(col(\"job_title\") == \"Biz Analyst\", \"Business Analyst\")\n",
    "    .when(col(\"job_title\") == \"Cloud Engr.\", \"Cloud Engineer\")\n",
    "    .when(col(\"job_title\") == \"DevOps Eng\", \"DevOps Engineer\")\n",
    "    .when(col(\"job_title\") == \"ML Engr.\", \"Machine Learning Engineer\")\n",
    "    .when(col(\"job_title\") == \"Fullstack Dev\", \"Full Stack Developer\")\n",
    "    .when(col(\"job_title\") == \"Cybersecurity Spec.\", \"Cybersecurity Analyst\")\n",
    "    .otherwise(col(\"job_title\"))\n",
    ")\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec6442e8-b573-43b2-83e5-7e396aa5c089",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Define location mapping\n",
    "location_mapping = {\n",
    "    \"NY, USA\": \"New York, USA\",\n",
    "    \"San Fran, US\": \"San Francisco, USA\",\n",
    "    \"London, United Kingdom\": \"London, UK\",\n",
    "    \"Berlin-Germany\": \"Berlin, Germany\",\n",
    "    \"Toronto-CA\": \"Toronto, Canada\",\n",
    "    \"Sydney/AUS\": \"Sydney, Australia\",\n",
    "    \"Mumbai, IND\": \"Mumbai, India\",\n",
    "    \"Singpore\": \"Singapore, Singapore\",\n",
    "    \"Tokyo-JP\": \"Tokyo, Japan\",\n",
    "    \"Paris FR\": \"Paris, France\"\n",
    "}\n",
    "\n",
    "# Create a UDF to map locations\n",
    "from pyspark.sql.functions import udf, col, split\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def map_location(location):\n",
    "    return location_mapping.get(location, location)\n",
    "\n",
    "map_location_udf = udf(map_location, StringType())\n",
    "\n",
    "# Apply location normalization\n",
    "df = df.withColumn(\"location\", map_location_udf(col(\"location\")))\n",
    "\n",
    "# Split location into city and country\n",
    "df = df.withColumn(\"city\", split(col(\"location\"), \", \")[0]) \\\n",
    "       .withColumn(\"country\", split(col(\"location\"), \", \")[1])\n",
    "\n",
    "# Drop the original location column\n",
    "df1 = df.drop(\"location\")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9be0295-1a0e-4da0-a6e8-acc25fde61e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Set the configuration to bypass the ANSI mode error\n",
    "spark.conf.set(\"spark.sql.ansi.enabled\", \"false\")\n",
    "\n",
    "from pyspark.sql.functions import col, to_date, when, date_format\n",
    "\n",
    "date_formats = [\n",
    "    \"MM-dd-yyyy\", \"yyyy/MM/dd\", \"dd MMM, yyyy\", \"MMM dd yyyy\",\n",
    "    \"yyyy-dd-MM\", \"dd-MM-yyyy\", \"yyyy.MM.dd\", \"MM/dd/yyyy\",\n",
    "    \"dd/MM/yy\", \"MMM/dd/yyyy\"\n",
    "]\n",
    "\n",
    "df = spark.table(\"workspace.default.raw_resume_data\")\n",
    "\n",
    "df = df.withColumn(\"normalized_date\", to_date(col(\"date\"), \"MM-dd-yyyy\"))\n",
    "\n",
    "for fmt in date_formats:\n",
    "    df = df.withColumn(\"normalized_date\", \n",
    "                       when(col(\"normalized_date\").isNull(), to_date(col(\"date\"), fmt))\n",
    "                       .otherwise(col(\"normalized_date\")))\n",
    "\n",
    "df = df.withColumn(\"final_date\", date_format(col(\"normalized_date\"), \"yyyy-MM-dd\")) \\\n",
    "       .drop(\"date\", \"normalized_date\") \\\n",
    "       .withColumnRenamed(\"final_date\", \"date\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc9160d-e06f-477c-830c-f1f692683bef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62df1465-5a22-4d62-9b62-3d79d24ec5d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df = df.drop('location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e497f3ed-5116-406d-a0cd-a6218c807b30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e4ecac8-13a2-4755-b2f5-f31c1e8642ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import col, when\n",
    "df1 = df.withColumn(\"job_title\",\n",
    "    when(col(\"job_title\") == \"Data Engr.\", \"Data Engineer\")\n",
    "    .when(col(\"job_title\") == \"Software Dev\", \"Software Developer\")\n",
    "    .when(col(\"job_title\") == \"Proj. Manager\", \"Project Manager\")\n",
    "    .when(col(\"job_title\") == \"Biz Analyst\", \"Business Analyst\")\n",
    "    .when(col(\"job_title\") == \"Cloud Engr.\", \"Cloud Engineer\")\n",
    "    .when(col(\"job_title\") == \"DevOps Eng\", \"DevOps Engineer\")\n",
    "    .when(col(\"job_title\") == \"ML Engr.\", \"Machine Learning Engineer\")\n",
    "    .when(col(\"job_title\") == \"Fullstack Dev\", \"Full Stack Developer\")\n",
    "    .when(col(\"job_title\") == \"Cybersecurity Spec.\", \"Cybersecurity Analyst\")\n",
    "    .otherwise(col(\"job_title\"))\n",
    ")\n",
    "display(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c61a890-0d24-4bcf-b1d5-d1c68fd20fae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "df1.createOrReplaceTempView(\"processed_resume_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80132f9-3f0a-4b22-b257-d4ecaf912219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "describe processed_resume_data"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "sql",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7083202003554999,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Resume Data Normalization",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
